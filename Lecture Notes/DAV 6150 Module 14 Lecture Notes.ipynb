{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# DAV 6150 Module 14: Automated Machine Learning Tools\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 13 Assignment Review + General Comments\n",
    "\n",
    "### Who had the best-performing / most robust Neural Network for the M13 Assignment?\n",
    "\n",
    "- Congrats to the team of __Yifeng Lin & Shichao Zhou__ on creating a feed-forward / back propagating neural network that __achieved accuracy + F1 scores that exceed 77.2%__ while __not overfitting the training data__.\n",
    "\n",
    "\n",
    "### How did they achieve this result?\n",
    "\n",
    "- Applying __PCA__ to numeric data => the 18 most significant principal components which cumulatively explain > 80% of the variance in the response variable were retained for modeling.\n",
    "\n",
    "\n",
    "- Using __SelectKBest()__ to identify the most relevant categorical attributes, which, according to their analysis, were:  __data_channel_is_world, is_weekend, data_channel_is_socmed, data_channel_is_entertainment, weekday_is_saturday, weekday_is_sunday, data_channel_is_tech__\n",
    "\n",
    "\n",
    "- Combining the 18 principal components with the 7 selected categorical attributes into a dataset to be used for model training/testing\n",
    "\n",
    "\n",
    "- Use of __Keras Sequential()__ function to construct 3 different feedforward/backpropagating networks configured with hyperparameter values chosen on the basis of their own reading/research.\n",
    "\n",
    "\n",
    "- Formulating their own __basic research question__ as part of the basis of their approach to determing the number of layers + nodes to implement, i.e., \"__Is increasing the number of hidden layers/neurons always giving better results?__\"\n",
    "\n",
    "\n",
    "- Training 3 networks using the following parameters:\n",
    "\n",
    "__Model 1__: 3 hidden layers. This model was found to __overfit__ the training data.\n",
    "\n",
    "- __layer 1__ = 100 nodes, relu activation. __Why is relu being used as the activation function__? \"We use Relu to speed up training. The computational step of Relu is easy because any negative elements are set to 0 and no exponentials, no multiplication or division operations. \"\n",
    "\n",
    "\n",
    "- __layer 2__ = 80 nodes, relu activation\n",
    "\n",
    "\n",
    "- __layer 3__ = 60 nodes, relu activation\n",
    "\n",
    "\n",
    "- __Output layer__ = 3 nodes, softmax activation. __Why softmax activation for the output layer__? \"Here, we did not use sigmoid because, in the study, we have three classes in the response variable. The softmax function is a more generalized logistic activation function used for multiclass classification that predicts a multinomial probability distribution.\"\n",
    "\n",
    "\n",
    "- __loss function__ =\"categorical_crossentropy\"; __Why categorical cross-entropy__? \"because the function is used in muti-class classfication tasks. In our study, there are three classes in our response variable.\"\n",
    "\n",
    "\n",
    "- __optimizer__ = SGD\n",
    "\n",
    "\n",
    "- __learning rate__ = 0.1. __How was the learning rate selected__? \"If a learning rate is so large, it can cause the model to learn faster and not predict anything accurately, while a smaller learning rate may allow the model to overfit. So here, we will use the default value 0.01 or 0.1.\"\n",
    "\n",
    "\n",
    "- __epochs__ = 30. __Why 30 epochs__? \"According to the results of our many experiments, we found that after more than 8 epochs, the model is over-fitting. In Model 1, we used 30 epochs as an example of overfitting. In Model 2, 10 epochs were selected. It shows that the fitting situation begins to appear in the second half. Based on the results of Model 2, Model 3 uses 7 epochs.\"\n",
    "\n",
    "\n",
    "__Model 2__: 3 hidden layers. This model was found to __partially overfit the training data__.\n",
    "\n",
    "- __layer 1__ = 100 nodes, relu activation. \n",
    "\n",
    "\n",
    "- __layer 2__ = 40 nodes, relu activation\n",
    "\n",
    "\n",
    "- __layer 3__ = 20 nodes, relu activation\n",
    "\n",
    "\n",
    "- __Output layer__ = 3 nodes, softmax activation. \n",
    "\n",
    "\n",
    "- __loss function__ =\"categorical_crossentropy\"\n",
    "\n",
    "\n",
    "- __optimizer__ = SGD\n",
    "\n",
    "\n",
    "- __learning rate__ = 0.01\n",
    "\n",
    "\n",
    "- __epochs__ = 10\n",
    "\n",
    "\n",
    "__Model 3__: 5 hidden layers. This model __does not overfit the training data__ and produced the performance metrics sighted above.\n",
    "\n",
    "- __layer 1__ = 100 nodes, relu activation. \n",
    "\n",
    "\n",
    "- __layer 2__ = 80 nodes, relu activation\n",
    "\n",
    "\n",
    "- __layer 3__ = 60 nodes, relu activation\n",
    "\n",
    "\n",
    "- __layer 4__ = 40 nodes, relu activation\n",
    "\n",
    "\n",
    "- __layer 5__ = 20 nodes, relu activation\n",
    "\n",
    "\n",
    "- __Output layer__ = 3 nodes, softmax activation. \n",
    "\n",
    "\n",
    "- __loss function__ =\"categorical_crossentropy\"\n",
    "\n",
    "\n",
    "- __optimizer__ = SGD\n",
    "\n",
    "\n",
    "- __learning rate__ = 0.01\n",
    "\n",
    "\n",
    "- __epochs__ = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Comments: What to Watch Out For Going Forward\n",
    "\n",
    "A fair number of Assignment + Project submits this semester (including the M13 Assignment) share similar, recurring approaches/practices that are either invalid or are not considered to be \"best practices\" in Data Science work\n",
    "\n",
    "### Not carefully reviewing / analyzing all attributes to determine whether they are numeric or categorical.\n",
    "\n",
    "Whenever we engage with a data set we should __ALWAYS__ carefully review any data dictionary that is provided + also __perform our own assessment / analysis__ of the data to identify whether an attribute should be treated as numeric data or categorical data. As we've seen many times this semester, oftentimes categorical data is provided in the form of digit-based categories which can easily be mistaken / misused as numeric data if we aren't careful. Failure to carefully review data at the start of your work can/will lead to invalid downstream analysis + conclusions that will damage your credibility as a practictioner.\n",
    "\n",
    "### Treating categorical data that is comprised of digits as numeric.\n",
    "\n",
    "__REMEMBER__: Categorical data is __QUALITATIVE__, not __QUANTITATIVE__. As such, it should not be analyzed using histograms or boxplots, and cannot be used as input to any pearson's correlation calculation.\n",
    "\n",
    "Also, prior to being used as input to any predictive model or machine learning algorithm, nominal categorical explanatory variables __need to be converted to dummy variables, even if they contain data values that are comprised of digits__. \n",
    "\n",
    "Categorical attributes should also not be used as input to any PCA. PCA requires the use of __numeric / quantitative__ attributes, and categorical data is clearly NOT quantitative.\n",
    "\n",
    "### Not analyzing data for preliminary predictive inferences during EDA + post-data prep revisit of EDA.\n",
    "\n",
    "In predictive modeling + machine learning work, one of the primary objectives of EDA work is to try to identify explanatory variables that appear likely to be predictive of a response variable. If your EDA does not include such analysis, it will not be credible.\n",
    "\n",
    "\n",
    "### Not proofreading your commentary to ensure it aligns with the output of your code\n",
    "\n",
    "Always double-check your commentary to ensure that what you've written accurately reflects the results of your analysis.\n",
    "\n",
    "\n",
    "### Including graphics or code within your analysis for which no commentary is provided or whose output is not used as part of your subsequent analysis.\n",
    "\n",
    "Graphics for which no commentary are provided serve no purpose to a reader of your work. If your work for some reason contains such \"stray\" graphics, they should be removed BEFORE you share your work with anyone else. The same rule of thumb applies to \"zombie\" code within your Notebooks whose output isn't being used as part of any subsequent analysis / computation.\n",
    "\n",
    "\n",
    "### Not providing a side-by-side comparison of the performance of your models\n",
    "\n",
    "Always provide a side-by-side comparison of your models, preferably in the form of a single summarial table that includes each of the performance metrics you've collected + the number of variables used as input to each of the models + any common hyperparameter settings that may vary from model to model.\n",
    "\n",
    "\n",
    "### If data scaling / standardization is required, it should be applied during your Data Preparation work BEFORE you split the data into training/testing subsets\n",
    "\n",
    "Any scaling or standardization of numeric data needs to be applied to the dataset as a whole __BEFORE__ it has been split into training/testing subsets for its effects to be consistent across both subsets.\n",
    "\n",
    "\n",
    "### PCA should be applied BEFORE a data set is split into training/testing subsets\n",
    "\n",
    "If applying PCA to numeric data, it needs to be applied to the dataset as a whole __BEFORE__ it has been split into training/testing subsets for its effects to be consistent across both subsets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Machine Learning Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A demonstration of the AWS SageMaker toolset will be provided by a guest speaker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
